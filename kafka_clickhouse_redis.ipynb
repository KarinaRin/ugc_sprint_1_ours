{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Работа с инфроструктурой"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.Останавливаем все контейнеры, если были запущены и заново их запускаем"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing control-center           ... \r\n",
      "Removing rest-proxy               ... \r\n",
      "Removing kafka-connect            ... \r\n",
      "Removing schema-registry          ... \r\n",
      "Removing broker                   ... \r\n",
      "Removing clickhouse-node1         ... \r\n",
      "Removing clickhouse-node3         ... \r\n",
      "Removing clickhouse-node4         ... \r\n",
      "Removing clickhouse-node2         ... \r\n",
      "Removing zookeeper_kafka          ... \r\n",
      "Removing zookeeper                ... \r\n",
      "Removing kafka_clickhouse_redis_1 ... \r\n",
      "\u001B[12BRemoving network kafka_clickhouse_defaulte\u001B[0m\u001B[8A\u001B[2K\r\n",
      "\"docker rm\" requires at least 1 argument.\r\n",
      "See 'docker rm --help'.\r\n",
      "\r\n",
      "Usage:  docker rm [OPTIONS] CONTAINER [CONTAINER...]\r\n",
      "\r\n",
      "Remove one or more containers\r\n",
      "Creating network \"kafka_clickhouse_default\" with the default driver\r\n",
      "Creating kafka_clickhouse_redis_1 ... \r\n",
      "Creating zookeeper                ... \r\n",
      "Creating zookeeper_kafka          ... \r\n",
      "\u001B[2BCreating clickhouse-node2         ... mdone\u001B[0m\r\n",
      "Creating clickhouse-node1         ... \r\n",
      "Creating clickhouse-node3         ... \r\n",
      "Creating clickhouse-node4         ... \r\n",
      "\u001B[7BCreating broker                   ... mdone\u001B[0m\r\n",
      "\u001B[1BCreating schema-registry          ... mdone\u001B[0m\u001B[4A\u001B[2K\u001B[1A\u001B[2K\r\n",
      "\u001B[1BCreating rest-proxy               ... mdone\u001B[0m\u001B[3A\u001B[2K\u001B[1A\u001B[2K\r\n",
      "Creating kafka-connect            ... \r\n",
      "\u001B[1BCreating control-center           ... mdone\u001B[0m\u001B[1A\u001B[2K\r\n",
      "\u001B[1Bting control-center           ... \u001B[32mdone\u001B[0m"
     ]
    }
   ],
   "source": [
    "!docker-compose down\n",
    "!docker rm $(docker ps -aq )\n",
    "!docker-compose up -d"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T17:14:02.157300231Z",
     "start_time": "2023-05-13T17:13:56.127576145Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Создаем таблицы в  clickhouse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from clickhouse_driver import Client\n",
    "\n",
    "client = Client(host='localhost')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T17:14:25.701607542Z",
     "start_time": "2023-05-13T17:14:25.540021681Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Создаем целевую таблицу, используя  Engine MergeTree. В MergeTree таблице будут храниться входные данные из кафки. Ожидаемый ответ []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_MergeTree = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS readings (\n",
    "    readings_id Int32 Codec(DoubleDelta, LZ4),\n",
    "    time DateTime Codec(DoubleDelta, LZ4),\n",
    "    date ALIAS toDate(time),\n",
    "    timestamp Int32 Codec(DoubleDelta, LZ4)\n",
    ") Engine = MergeTree\n",
    "PARTITION BY toYYYYMM(time)\n",
    "ORDER BY (readings_id, time);\n",
    "\"\"\"\n",
    "client.execute(create_MergeTree)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T17:14:27.642512527Z",
     "start_time": "2023-05-13T17:14:27.604537504Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Далее нам нужно создать таблицу с помощью движка Kafka для подключения к топику и чтения данных. Движок будет считывать данные с брокера на хосте kafka, из топика «readings», имя группы потребителей  - «readings consumer_group1». Входной формат — CSV. Обратите внимание, что столбец «дата» опущен. Это псевдоним в целевой таблице, который будет автоматически заполняться из столбца «время». Ожидаемый ответ []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_kafka_engine = \"\"\"\n",
    "CREATE TABLE readings_queue (\n",
    "    readings_id Int32,\n",
    "    time DateTime,\n",
    "    timestamp Int32\n",
    ")\n",
    "ENGINE = Kafka\n",
    "SETTINGS kafka_broker_list = 'broker:29092',\n",
    "       kafka_topic_list = 'readings',\n",
    "       kafka_group_name = 'readings_consumer_group1',\n",
    "       kafka_format = 'CSV',\n",
    "       kafka_max_block_size = 1048576;\n",
    "\"\"\"\n",
    "client.execute(create_kafka_engine)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T17:14:30.963498506Z",
     "start_time": "2023-05-13T17:14:30.919413594Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Материализованное представление соединит две ранее созданные таблицы, считывая данные из механизма таблиц Kafka и вставляя их в целевую таблицу дерева слияния. Мы можем сделать ряд преобразований данных. Мы сделаем простое чтение и вставку. Использование * предполагает, что имена столбцов идентичны (с учетом регистра). Ожидаемый ответ []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_materialized_view = \"\"\"\n",
    "    CREATE MATERIALIZED VIEW readings_queue_mv TO readings AS\n",
    "    SELECT readings_id, time, timestamp\n",
    "    FROM readings_queue;\n",
    "\"\"\"\n",
    "client.execute(create_materialized_view)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T17:14:33.261248133Z",
     "start_time": "2023-05-13T17:14:33.251922739Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Проверяем доступные таблицы. Ожидаемый ответ [('readings',), ('readings_queue',), ('readings_queue_mv',)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "[('readings',), ('readings_queue',), ('readings_queue_mv',)]"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.execute('SHOW TABLES FROM default')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T18:11:22.806949586Z",
     "start_time": "2023-05-13T18:11:22.765000760Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Проверяем что сейчас данных нет. Ожидаемый ответ []"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "[(6, datetime.datetime(2023, 5, 13, 20, 15, 7), 1683964369),\n (1, datetime.datetime(2023, 5, 13, 20, 14, 51), 1683964364),\n (2, datetime.datetime(2023, 5, 13, 20, 14, 54), 1683964365),\n (3, datetime.datetime(2023, 5, 13, 20, 14, 58), 1683964366),\n (4, datetime.datetime(2023, 5, 13, 20, 15, 1), 1683964367),\n (5, datetime.datetime(2023, 5, 13, 20, 15, 4), 1683964368)]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data_from_clickhouse = \"\"\"\n",
    "SELECT * FROM readings\n",
    "\"\"\"\n",
    "\n",
    "client.execute(get_data_from_clickhouse)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T18:12:14.488388487Z",
     "start_time": "2023-05-13T18:12:14.438398861Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Создаем коннектор kafka -> redis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Отправляем файл конфигурации на коннектор (kafka-connect в docker-compose). Ожидаемый ответ: {\"name\":\"RedisSinkConnector1\",\"config\":{\"connector.class\":\"com.github.jcustenborder.kafka.connect.redis.RedisSinkConnector\",\"tasks.max\":\"1\",\"topics\":\"readings\",\"redis.hosts\":\"redis:6379\",\"key.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"value.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"name\":\"RedisSinkConnector1\"},\"tasks\":[],\"type\":\"sink\"}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"RedisSinkConnector1\",\"config\":{\"connector.class\":\"com.github.jcustenborder.kafka.connect.redis.RedisSinkConnector\",\"tasks.max\":\"1\",\"topics\":\"readings\",\"redis.hosts\":\"redis:6379\",\"key.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"value.converter\":\"org.apache.kafka.connect.storage.StringConverter\",\"name\":\"RedisSinkConnector1\"},\"tasks\":[],\"type\":\"sink\"}"
     ]
    }
   ],
   "source": [
    "!curl -X POST -H 'Content-Type: application/json' --data @connector.json http://localhost:8083/connectors"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-13T17:14:41.556512484Z",
     "start_time": "2023-05-13T17:14:41.127542704Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Запускаем файл kafka_producer_redis_consumer.py\n",
    "# Смотрим lighthouse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
